{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Example Notebook\n",
    "This notebook provides several examples of how to use nn.LSTM. \n",
    "\n",
    "# Sequence Example\n",
    "Based on the example here: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3, 2) # Input dim is 3, Output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(2)] # make a sequence of length 5\n",
    "print(f'[INPUTS] shape: ({len(inputs)}, {inputs[0].shape})')\n",
    "print(f'[INPUTS] elements: {inputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- The first element of the hidden shape must be the same as the `num_layers` in the LSTM\n",
    "- The third element of the hidden shape mush be the same as the `hidden_size` in the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(2, 1, 3), \n",
    "          torch.randn(2, 1, 3))\n",
    "print(f'[HIDDEN] shape: ({len(hidden)}, {hidden[0].shape})')\n",
    "print(f'[HIDDEN] elements: {hidden}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    # step through the sequence on element at a time\n",
    "    # after each step, hidden contains the hidden state\n",
    "    print(f'[INPUT] {i.shape}, {i}')\n",
    "    reshapped_i = i.view(1, 1, -1)\n",
    "    print(f'[I VIEW] {reshapped_i.shape}, element: {reshapped_i}')\n",
    "    out, hidden = lstm(reshapped_i, hidden)\n",
    "    print(f'[OUT] {out.shape}, elements: {out}')\n",
    "    print(f'[HIDDEN] ({len(hidden)},{hidden[0].shape}), elements: {hidden}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can do the entire sequence all at once\n",
    "# The first value returned by the LSTM is all of the hidden states throughout\n",
    "# the sequence. The second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, then are the same)\n",
    "# The reason for this is:\n",
    "# \"out\" will give you access to all hidden states in the sequence, \n",
    "# \"hidden\" will allow you to continue the sequence and backpropogate,\n",
    "# by passing it as an arguement to the LSTM at a later time\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "print(f'[INPUTS] shape: {inputs.shape}')\n",
    "hidden = (torch.randn(2, 1, 3), torch.randn(2, 1, 3)) # Clean out the hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(f'[OUT] {out}')\n",
    "print(f'[HIDDEN] {hidden}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Part-of-Speach Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybod read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "print(training_data[0][0], training_data)\n",
    "\n",
    "word_to_idx = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "print(word_to_idx)\n",
    "tag_to_idx = {\"DET\": 0, \"NN\":1, \"V\":2}\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 6 # usually something like 32 or 64\n",
    "HIDDEN_DIM = 6 # usually something like 32 or 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        print(vocab_size, embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        print(self.word_embeddings)\n",
    "        # the lstm takes word embeddings as input and outputs hidden state\n",
    "        # with dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        print(sentence.shape)\n",
    "        print(f'[EMBEDS] {embeds.shape}')\n",
    "        transformed = embeds.view(len(sentence), 1, -1)\n",
    "        print(transformed.shape, transformed)\n",
    "        lstm_out, _ = self.lstm(transformed)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_idx)\n",
    "    print('[INPUTS]', inputs.shape, inputs)\n",
    "    tag_score = model(inputs)\n",
    "    print(tag_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tag in training_data:\n",
    "        model.zero_grad()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        targets = prepare_sequence(tag, tag_to_idx)\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_idx)\n",
    "    sentence = \"\"\n",
    "    for i in inputs:\n",
    "        for key, value in word_to_idx.items():\n",
    "            if i == value:\n",
    "                sentence += key+\" \"\n",
    "    print(sentence)\n",
    "    tag_scores = model(inputs)\n",
    "    print(f'[INPUTS] {inputs}')\n",
    "    predictions = torch.argmax(tag_scores, dim=1)\n",
    "    tags = \"\"\n",
    "    for p in predictions:\n",
    "        for key, value in tag_to_idx.items():\n",
    "            if p == value:\n",
    "                tags += key + \" \"\n",
    "\n",
    "    print(tags)\n",
    "    print(f'[PREDICTIONS] {predictions}')\n",
    "    print(tag_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSP RL LSTM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvironmentEncoder(\n",
      "  (embedding): Linear(in_features=5, out_features=256, bias=True)\n",
      "  (ee): LSTM(256, 256, num_layers=5, batch_first=True)\n",
      "  (meta_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "(10, 5)\n",
      "[ACTIONS] tensor([[[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]],\n",
      "\n",
      "        [[-0.0759, -0.0507, -0.0458]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from gsp_rl.src.networks import EnvironmentEncoder, DQN\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "\n",
    "lstm_nn_args = {\n",
    "    'lr':1e-5,\n",
    "    'input_size':5,\n",
    "    'output_size':1,\n",
    "    'embedding_size':256,\n",
    "    'hidden_size':256,\n",
    "    'num_layers':5,\n",
    "    'batch_size':16\n",
    "}\n",
    "dqn_nn_args = {\n",
    "    'id': 1, \n",
    "    'lr': 1e-5,\n",
    "    'input_size': lstm_nn_args['output_size'],\n",
    "    'output_size': 3,\n",
    "}\n",
    "lstm_actor = EnvironmentEncoder(**lstm_nn_args)\n",
    "actor = DQN(**dqn_nn_args)\n",
    "print(lstm_actor)\n",
    "\n",
    "testing_data = [torch.randn((5)) for _ in range(10)]\n",
    "testing_data = np.array(testing_data)\n",
    "print(testing_data.shape)\n",
    "out = lstm_actor(torch.tensor(testing_data))\n",
    "actions = actor(out)\n",
    "\n",
    "#print(out)\n",
    "print('[ACTIONS]', actions)\n",
    "# embedding  = nn.Linear(5, 10)\n",
    "# sample_in = torch.randn(5)\n",
    "# print(sample_in.shape, sample_in)\n",
    "# out = embedding(sample_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (663847573.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 41\u001b[0;36m\u001b[0m\n\u001b[0;31m    testing_data = torch.randn((lstm_nn_args['input_size'])) for _ in range(10)\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from gsp_rl.src.networks import EnvironmentEncoder, DDPGActorNetwork, DDPGCriticNetwork, RDDPGCriticNetwork, RDDPGActorNetwork\n",
    "import torch\n",
    "import numpy as np\n",
    "lstm_nn_args = {\n",
    "    'lr':1e-5,\n",
    "    'input_size':5,\n",
    "    'output_size':256,\n",
    "    'embedding_size':200,\n",
    "    'hidden_size':300,\n",
    "    'num_layers':5,\n",
    "    'batch_size':16\n",
    "}\n",
    "\n",
    "ddpg_actor_nn_args = {\n",
    "    'id': 1,\n",
    "    'lr': 1e-4,\n",
    "    'input_size':lstm_nn_args['output_size'],\n",
    "    'output_size': 2,\n",
    "    'fc1_dims': 400,\n",
    "    'fc2_dims': 300\n",
    "}\n",
    "ddpg_critic_nn_args = {\n",
    "    'id': 1,\n",
    "    'lr': 1e-4,\n",
    "    'input_size':lstm_nn_args['output_size'] + ddpg_actor_nn_args['output_size'],\n",
    "    'output_size': 2,\n",
    "    'fc1_dims': 400,\n",
    "    'fc2_dims': 300\n",
    "}\n",
    "\n",
    "def makeRDDPG(lstm_nn_args, ddpg_actor_nn_args, ddpg_critic_nn_args):\n",
    "    ee = EnvironmentEncoder(**lstm_nn_args)\n",
    "    actor = DDPGActorNetwork(**ddpg_actor_nn_args)\n",
    "    critic = DDPGCriticNetwork(**ddpg_critic_nn_args)\n",
    "    rddpg_actor = RDDPGActorNetwork(ee, actor)\n",
    "    rddpg_critic = RDDPGCriticNetwork(ee, critic)\n",
    "    return rddpg_actor, rddpg_critic\n",
    "\n",
    "actor, critic = makeRDDPG(lstm_nn_args, ddpg_actor_nn_args, ddpg_critic_nn_args)\n",
    "ee = EnvironmentEncoder(**lstm_nn_args)\n",
    "testing_data = [torch.randn((lstm_nn_args['input_size'])) for _ in range(10)]\n",
    "testing_data = torch.tensor(np.array(testing_data))\n",
    "np_data = [np.random.randint(1, 25, lstm_nn_args['input_size']) for _ in range(10)]\n",
    "np_data = np.array(np_data)\n",
    "print(testing_data.shape, np_data.shape)\n",
    "action = actor(testing_data)\n",
    "# print(action)\n",
    "# print(action.view(testing_data.shape[0], 2))\n",
    "# print(testing_data)\n",
    "torch.cat([testing_data, action.view(testing_data.shape[0], 2)], dim=-1)\n",
    "# print(testing_data.shape, action.shape)\n",
    "\n",
    "value = critic(testing_data, action)\n",
    "# print(value)\n",
    "\n",
    "# for name, param in ee.named_parameters():\n",
    "#     print(name, param.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsp_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
